# -*- coding: utf-8 -*-
"""FP Data Mining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I3hlOlpgykwY3POh50UFVxcNh1RmphC4

#Scraping Data
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

"""##Merge Data"""

asumsico_data = pd.read_csv('/content/dataset_asumsico.csv')
bisniscom_data = pd.read_csv('/content/dataset_bisniscom.csv')
folkative_data = pd.read_csv('/content/dataset_folkative.csv')
jogdaily_data = pd.read_csv('/content/dataset_jogdaily.csv')
katadata_data = pd.read_csv('/content/dataset_katadata.csv')
kepri_data = pd.read_csv('/content/dataset_kepri.csv')
pacitanku_data = pd.read_csv('/content/dataset_pacitanku.csv')
seputaranlpk_data = pd.read_csv('/content/dataset_seputaranplk.csv')

merged_df = pd.concat([asumsico_data, bisniscom_data, folkative_data, jogdaily_data, katadata_data, kepri_data, pacitanku_data, seputaranlpk_data], ignore_index=True)
merged_df = merged_df.iloc[:, 1:]
merged_df.to_csv('dataset_fp.csv', index = False)

data = pd.read_csv('/content/dataset_fp.csv')

data

"""#Preprocess Data (Awal)"""

import pandas as pd
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/dataset_fp.csv')

data.head()

print(data.head(5))
print('----------')
print('Shape: ' , data.shape)
print('----------')
print(data.dtypes)
print('----------')
print(data.describe())
print('----------')
print('Data Null?')
print(data.isnull().sum())

def check_duplicate_rows(data) :
  duplicate_rows = data[data.duplicated(keep=False)]
  return duplicate_rows

duplicate_rows = check_duplicate_rows(data)

if not duplicate_rows.empty:
  print("Baris duplikat ditemukan:")
  print(duplicate_rows)
else:
  print("Tidak ditemukan baris duplikat.")

def remove_duplication(data):
  data.drop_duplicates(subset=['text'], keep='first', inplace=True)
  return data

data = remove_duplication(data)
data.shape

def check_words(data):
  short_reviews = data[data['text'].str.split().str.len() < 3]
  return short_reviews

short_reviews = check_words(data)

if not short_reviews.empty:
  print("Baris dengan review kurang dari 3 kata:")
  print(short_reviews)
else:
  print("Tidak ditemukan baris dengan review kurang dari 3 kata.")

def remove_short_reviews(data):
  data = data[data['text'].str.split().str.len() >= 3]
  return data

data = remove_short_reviews(data)
data.shape

data.to_csv('cleaned_data.csv', index=False)

"""#Convert Bahasa"""

data = pd.read_csv('/content/cleaned_data.csv')
data.head()

"""##Translate Data Text"""

!pip install googletrans==3.1.0a0

from googletrans import Translator

translator = Translator()

def translate_to_english(text):
    try:
        translated = translator.translate(text, dest='en')
        return translated.text
    except Exception as e:
        print(f"Translation error: {e}")
        return text

data['convert_text'] = data['text'].apply(translate_to_english)

print(data[['account', 'username', 'text', 'convert_text']].head())

# Menyimpan data dengan kolom baru ke file baru
output_file = 'translated_data.csv'
data.to_csv(output_file, index=False)

print(f"File dengan kolom sentimen berhasil disimpan di {output_file}")

data.head()

"""#Pelabelan"""

data = pd.read_csv('/content/translated_data.csv')
data.head()

"""##Vader"""

import pandas as pd
from nltk.sentiment import SentimentIntensityAnalyzer
import nltk

nltk.download('vader_lexicon')

sia = SentimentIntensityAnalyzer()

# Fungsi untuk menentukan sentimen
def get_sentiment(review):
    scores = sia.polarity_scores(review)
    # Menggunakan nilai compound untuk menentukan sentimen
    compound_score = scores['compound']
    if compound_score > 0.05:
        return 'positif'
    elif compound_score < -0.05:
        return 'negatif'
    else:
        return 'netral'

data['vader_sentimen'] = data['convert_text'].apply(get_sentiment)
#data = data.drop(columns=['text', 'normalized_text'])

data.head()

"""##TextBlob"""

!pip install pandas textblob

from textblob import TextBlob

def get_sentiment(review):
    analysis = TextBlob(review)
    polarity = analysis.sentiment.polarity
    if polarity > 0:
        return 'positif'
    elif polarity < 0:
        return 'negatif'
    else:
        return 'netral'

data['textblob_sentimen'] = data['convert_text'].apply(get_sentiment)
#data = data.drop(columns=['convert_text'])

#output_file = 'labeled_data.csv'
#data.to_csv(output_file, index=False)

data.head()

"""##Afinn"""

!pip install afinn

from afinn import Afinn

afinn = Afinn()

def get_afinn_sentiment(text):
    score = afinn.score(text)
    if score > 0:
        return 'positif'
    elif score < 0:
        return 'negatif'
    else:
        return 'netral'

data['afinn_sentimen'] = data['convert_text'].apply(get_afinn_sentiment)

output_file = 'labeled_data.csv'
data.to_csv(output_file, index=False)

print(data[['account', 'username', 'text', 'vader_sentimen', 'textblob_sentimen', 'afinn_sentimen']].head())
#print(f"File dengan label sentimen berhasil disimpan di {output_file}")

data.head()

"""##Git Labeling (Hapus)"""

import pandas as pd
import requests

positive_url = "https://raw.githubusercontent.com/fajri91/InSet/master/positive.tsv"
negative_url = "https://raw.githubusercontent.com/fajri91/InSet/master/negative.tsv"

positive_words = pd.read_csv(positive_url, sep='\t', header=None, names=['word'])
negative_words = pd.read_csv(negative_url, sep='\t', header=None, names=['word'])

positive_words['sentiment'] = 'positif'
negative_words['sentiment'] = 'negatif'

# Menggabungkan kedua dataset menjadi satu kamus sentimen
sentiment_dict = pd.concat([positive_words, negative_words], ignore_index=True)

# Mengubah DataFrame menjadi dictionary
sentiment_mapping = dict(zip(sentiment_dict['word'], sentiment_dict['sentiment']))

def label_sentiment(text):
    tokens = text.split()
    sentiment_labels = [sentiment_mapping.get(word, 'netral') for word in tokens]
    if 'negatif' in sentiment_labels:
        return 'negatif'
    elif 'positif' in sentiment_labels:
        return 'positif'
    return 'netral'

data['git_sentimen'] = data['text'].apply(label_sentiment)

#output_file = 'labeled_sentiment_output.csv'
#data.to_csv(output_file, index=False)


print(data[['account', 'username', 'text', 'vader_sentimen', 'textblob_sentimen', 'git_sentimen']].head())

#print(f"File dengan label sentimen berhasil disimpan di {output_file}")

data.head(20)

"""##SentiWordNet (Hapus)"""

!pip install sentiwordnet

from nltk.corpus import sentiwordnet as swn
nltk.download('sentiwordnet')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

def sentiwordnet_sentiment(text):
    tokens = word_tokenize(text)
    lemmas = [lemmatizer.lemmatize(token) for token in tokens]
    sentiment_score = 0
    for lemma in lemmas:
        synsets = list(swn.senti_synsets(lemma))
        if synsets:
            synset = synsets[0]
            sentiment_score += synset.pos_score() - synset.neg_score()
    if sentiment_score > 0.1:
        return 'positif'
    elif sentiment_score < -0.1:
        return 'negatif'
    else:
        return 'netral'

data['sentiwordnet_sentimen'] = data['convert_text'].apply(sentiwordnet_sentiment)
data = data.drop(columns=['convert_text'])

output_file = 'netral_labeled_data.csv'
data.to_csv(output_file, index=False)

print(data[['account', 'username', 'text', 'vader_sentimen', 'textblob_sentimen', 'git_sentimen', 'afinn_sentimen', 'sentiwordnet_sentimen']].head())
print(f"File dengan label sentimen berhasil disimpan di {output_file}")

"""#Visualisasi Distribusi Data"""

data = pd.read_csv('/content/labeled_data.csv')
data.head()

data = pd.read_csv('/content/drive/MyDrive/Data Analyst/FP Data Mining/labeled_data.csv')
data.head(20)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sentiment_columns = ['vader_sentimen', 'textblob_sentimen', 'afinn_sentimen']

plt.figure(figsize=(15, 10))

for i, col in enumerate(sentiment_columns):
    plt.subplot(2, 3, i + 1)
    sns.countplot(x=col, data=data)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')

plt.tight_layout()
plt.show()

vader_negative_count = data[data['vader_sentimen'] == 'negatif'].shape[0]
vader_positive_count = data[data['vader_sentimen'] == 'positif'].shape[0]
vader_netral_count = data[data['vader_sentimen'] == 'netral'].shape[0]

textblob_negative_count = data[data['textblob_sentimen'] == 'negatif'].shape[0]
textblob_positive_count = data[data['textblob_sentimen'] == 'positif'].shape[0]
textblob_netral_count = data[data['textblob_sentimen'] == 'netral'].shape[0]

afinn_negative_count = data[data['afinn_sentimen'] == 'negatif'].shape[0]
afinn_positive_count = data[data['afinn_sentimen'] == 'positif'].shape[0]
afinn_netral_count = data[data['afinn_sentimen'] == 'netral'].shape[0]

print("Vader Sentiment Counts:")
print(f"  Negative: {vader_negative_count}")
print(f"  Positive: {vader_positive_count}")
print(f"  Netral: {vader_netral_count}")

print("\nTextBlob Sentiment Counts:")
print(f"  Negative: {textblob_negative_count}")
print(f"  Positive: {textblob_positive_count}")
print(f"  Netral: {textblob_netral_count}")

print("\nAfinn Sentiment Counts:")
print(f"  Negative: {afinn_negative_count}")
print(f"  Positive: {afinn_positive_count}")
print(f"  Netral: {afinn_netral_count}")

"""#Pengambilan Nilai Dominan"""

import pandas as pd
from collections import Counter

def get_final_sentiment(row):
    sentiments = [row['vader_sentimen'], row['textblob_sentimen'], row['afinn_sentimen']]
    sentiment_counts = Counter(sentiments)  # Hitung frekuensi masing-masing sentimen
    most_common_sentiment = sentiment_counts.most_common(1)[0]  # Ambil sentimen dominan

    # Jika mayoritas sentimen muncul dua kali atau lebih
    if most_common_sentiment[1] >= 2:
        return most_common_sentiment[0]
    return 'equal'

data['final_sentiment'] = data.apply(get_final_sentiment, axis=1)

data = data.drop(columns=['vader_sentimen', 'textblob_sentimen', 'afinn_sentimen', 'convert_text'])
output_file = 'final_sentiment_data.csv'
data.to_csv(output_file, index=False)

print(data[['account', 'username', 'text', 'final_sentiment']].head())
print(f"Data dengan sentimen final disimpan di {output_file}")

"""#Eksplorasi Data"""

import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv('/content/final_sentiment_data.csv')
data.head()

"""#Pengecekan Distribusi Sumber Data"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

sentiment_counts = data['account'].value_counts()
account = sentiment_counts.index

plt.figure(figsize=(10, 5))
bars = plt.bar(account, sentiment_counts, color='skyblue')
plt.title("Distribution of Accounts")
plt.xlabel("Account")
plt.ylabel("Jumlah Comment")

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.1, round(yval, 2), ha='center', va='bottom')

plt.xticks(rotation=45)
plt.show()

"""##Pengecekan Distribusi Kelas"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

sentiment_counts = data['final_sentiment'].value_counts()

plt.figure(figsize=(8, 6))
sentiment_counts.plot(kind='pie', color=['skyblue', 'lightcoral', 'lightgreen', 'orange', 'lightgrey'], autopct='%1.1f%%', startangle=90)

plt.title('Distribution of Sentiment')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.tight_layout()

plt.show()

"""##Pengecekan Kata yang Paling Banyak Muncul"""

from wordcloud import WordCloud

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(data['text']))

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Comment Text')
plt.show()

"""#Preprocessing"""

import re
import string
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt_tab')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

data = pd.read_csv('/content/final_sentiment_data.csv')
data.head()

"""##Text Cleaning"""

def clean_text(text):
  text = text.lower()  # Mengubah teks menjadi huruf kecil
  text = re.sub(r'[^a-zA-Z\s]', '', text)  # Menghapus karakter khusus dan angka
  text = re.sub(r'[^\w\s]', '', text) # Menghapus karakter non-alphanumeric dan whitespace
  text = re.sub(r'\s+', ' ', text).strip()  # Menghapus whitespace ekstra
  return text

data['cleaned_text'] = data['text'].apply(clean_text)

comparison_df = data[['account', 'username', 'text', 'cleaned_text', 'final_sentiment']]
print(comparison_df.head(10))

#comparison_df.to_csv('cleaned_comparison.csv', index=False)

"""##Tokenisasi"""

def tokenize_text(text):
  return word_tokenize(text)

data['tokenized_text'] = data['cleaned_text'].apply(tokenize_text)

comparison_tokenized_df = data[['account', 'username', 'text', 'tokenized_text', 'final_sentiment']]
print(comparison_tokenized_df.head(10))

#comparison_tokenized_df.to_csv('tokenized_comparison.csv', index=False)

"""##Normalisasi"""

normalizad_word = pd.read_csv('/content/kamusalay.csv', header=None)  # Memuat kamus alay (file .csv kamus normalisasi)
normalizad_word_dict = {row[0]: row[1] for _, row in normalizad_word.iterrows()}  # Membuat kamus normalisasi dari csv

def normalize_term(tokens):
    return [normalizad_word_dict.get(term, term) for term in tokens]  # Jika term ada di kamus, normalisasikan, jika tidak tetap

data['normalized_text'] = data['tokenized_text'].apply(normalize_term)

comparison_normalization_df = data[['account', 'username', 'text', 'normalized_text', 'final_sentiment']]
print(comparison_normalization_df.head(10))

#comparison_normalization_df.to_csv('normalized_comparison.csv', index=False)

"""##Stopword Removal"""

stop_words = set(stopwords.words('indonesian')) #menghapus kata yang tidak memberikan informasi signifikan (ini, itu, dll)

def remove_stopwords(tokens):
  filtered_tokens = [token for token in tokens if token not in stop_words]
  return filtered_tokens

data['filtered_text'] = data['normalized_text'].apply(remove_stopwords)

comparison_stopwords_df = data[['account', 'username', 'text', 'normalized_text', 'filtered_text', 'final_sentiment']]
print(comparison_stopwords_df.head(10))

#comparison_stopwords_df.to_csv('stopwords_removal_comparison.csv', index=False)

"""##Stemming"""

!pip install Sastrawi

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

factory = StemmerFactory()
stemmer = factory.create_stemmer()

def stemming_text(tokens):
  stemmed_tokens = [stemmer.stem(token) for token in tokens]
  return stemmed_tokens

data['stemmed_text'] = data['filtered_text'].apply(stemming_text)

comparison_stemming_df = data[['account', 'username', 'text', 'normalized_text', 'filtered_text', 'stemmed_text', 'final_sentiment']]
print(comparison_stemming_df.head(10))

comparison_stemming_df.to_csv('stemming_comparison.csv', index=False)

data = pd.read_csv('/content/stemming_comparison.csv')
data.rename(columns={'stemmed_text': 'preprocessed_text'}, inplace=True)
data = data.drop(columns=['normalized_text', 'filtered_text'])
data.head(20)

data = data.to_csv('final_dataset.csv', index=False)

"""#Resampling dan Splitting Data"""

data = pd.read_csv('/content/final_dataset.csv')
data.head()

import pandas as pd

data = data[data['final_sentiment'] != 'netral']
data = data[data['final_sentiment'] != 'equal']
data

final_sentiment_counts = data['final_sentiment'].value_counts()
final_sentiment_counts

from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE

"""##80:20

###Splitting Data
"""

train_data_80, test_data_20 = train_test_split(
    data, test_size=0.2, random_state=42, stratify=data['final_sentiment'] #Menjaga distribusi label kelas
    )

data.head()

"""###Resampling"""

#Random Under Sampling
us_80 = RandomUnderSampler(random_state=42)
train_features_us_80, train_labels_us_80 = us_80.fit_resample(
    train_data_80[['preprocessed_text']], train_data_80['final_sentiment']
    )

#Random Over Sampling
os_80 = RandomOverSampler(random_state=42)
train_features_os_80, train_labels_os_80 = os_80.fit_resample(
    train_data_80[['preprocessed_text']], train_data_80['final_sentiment']
    )

"""###70:30"""

train_data_70, test_data_30 = train_test_split(
    data, test_size=0.3, random_state=42, stratify=data['final_sentiment'] #Menjaga distribusi label kelas
    )

data.head()

"""###Resampling"""

#Random Under Sampling
us_70 = RandomUnderSampler(random_state=42)
train_features_us_70, train_labels_us_70 = us_70.fit_resample(
    train_data_70[['preprocessed_text']], train_data_70['final_sentiment']
    )

#Random Over Sampling
os_70 = RandomOverSampler(random_state=42)
train_features_os_70, train_labels_os_70 = os_70.fit_resample(
    train_data_70[['preprocessed_text']], train_data_70['final_sentiment']
    )

"""##Hasil"""

print("Splitting 80:20:")
print(f"Train Data Shape (80%): {train_data_80.shape}")
print(f"Test Data Shape (20%): {test_data_20.shape}")
print(f"Train Features US Shape (80:20): {train_features_us_80.shape}")
print(f"Train Labels US Shape (80:20): {train_labels_us_80.shape}")
print(f"Train Features OS Shape (80:20): {train_features_os_80.shape}")
print(f"Train Labels OS Shape (80:20): {train_labels_os_80.shape}")

print("\nSplitting 70:30:")
print(f"Train Data Shape (70%): {train_data_70.shape}")
print(f"Test Data Shape (30%): {test_data_30.shape}")
print(f"Train Features US Shape (70:30): {train_features_us_70.shape}")
print(f"Train Labels US Shape (70:30): {train_labels_us_70.shape}")
print(f"Train Features OS Shape (70:30): {train_features_os_70.shape}")
print(f"Train Labels OS Shape (70:30): {train_labels_os_70.shape}")

"""#Term Weighting (TF-IDF)
Note: kolom preprocessed masih berupa list
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

#Mengubah kolom preprocessed menjadi string
def convert_list_to_string(dataframe, column_name):
    dataframe[column_name] = dataframe[column_name].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))
    return dataframe

#Convert kolom preprocessed_text
train_data_80 = convert_list_to_string(train_data_80, 'preprocessed_text')
test_data_20 = convert_list_to_string(test_data_20, 'preprocessed_text')

train_data_70 = convert_list_to_string(train_data_70, 'preprocessed_text')
test_data_30 = convert_list_to_string(test_data_30, 'preprocessed_text')

train_features_os_80 = convert_list_to_string(train_features_os_80, 'preprocessed_text') #oversampling
train_features_us_80 = convert_list_to_string(train_features_us_80, 'preprocessed_text') #undersampling

train_features_os_70 = convert_list_to_string(train_features_os_70, 'preprocessed_text')
train_features_us_70 = convert_list_to_string(train_features_us_70, 'preprocessed_text')

"""##80:20"""

#Normal
tfidf_vectorizer_normal_80 = TfidfVectorizer(ngram_range=(1, 2))
tfidf_features_normal_80 = tfidf_vectorizer_normal_80.fit_transform(train_data_80['preprocessed_text']) #data train
tfidf_features_test_normal_80 = tfidf_vectorizer_normal_80.transform(test_data_20['preprocessed_text']) #data test

#Random Under Sampling
tfidf_vectorizer_us_80 = TfidfVectorizer(ngram_range=(1, 2))
tfidf_features_us_80 = tfidf_vectorizer_us_80.fit_transform(train_features_us_80['preprocessed_text'])
tfidf_features_test_us_80 = tfidf_vectorizer_us_80.transform(test_data_20['preprocessed_text'])

#Random Over Sampling
tfidf_vectorizer_os_80 = TfidfVectorizer(ngram_range=(1, 2))
tfidf_features_os_80 = tfidf_vectorizer_os_80.fit_transform(train_features_os_80['preprocessed_text'])
tfidf_features_test_os_80 = tfidf_vectorizer_os_80.transform(test_data_20['preprocessed_text'])

"""##70:30"""

#Normal
tfidf_vectorizer_normal_70 = TfidfVectorizer(ngram_range=(1, 2))
tfidf_features_normal_70 = tfidf_vectorizer_normal_70.fit_transform(train_data_70['preprocessed_text'])
tfidf_features_test_normal_70 = tfidf_vectorizer_normal_70.transform(test_data_30['preprocessed_text'])

#Random Under Sampling
tfidf_vectorizer_us_70 = TfidfVectorizer(ngram_range=(1, 2))
tfidf_features_us_70 = tfidf_vectorizer_us_70.fit_transform(train_features_us_70['preprocessed_text'])
tfidf_features_test_us_70 = tfidf_vectorizer_us_70.transform(test_data_30['preprocessed_text'])

#Random Over Sampling
tfidf_vectorizer_os_70 = TfidfVectorizer(ngram_range=(1, 2))
tfidf_features_os_70 = tfidf_vectorizer_os_70.fit_transform(train_features_os_70['preprocessed_text'])
tfidf_features_test_os_70 = tfidf_vectorizer_os_70.transform(test_data_30['preprocessed_text'])

"""##Aplikasi SMOTE"""

#Splitting 80:20
smote_80 = SMOTE(random_state=42)
tfidf_features_smote_80, labels_smote_80 = smote_80.fit_resample(
    tfidf_features_normal_80, train_data_80['final_sentiment']
)

#Splitting 70:30
smote_70 = SMOTE(random_state=42)
tfidf_features_smote_70, labels_smote_70 = smote_70.fit_resample(
    tfidf_features_normal_70, train_data_70['final_sentiment']
)

"""##Hasil"""

print("TF-IDF Feature Extraction Complete.")
print(f"Normal 80:20 - Train Features: {tfidf_features_normal_80.shape}, Test Features: {tfidf_features_test_normal_80.shape}")
print(f"Normal 70:30 - Train Features: {tfidf_features_normal_70.shape}, Test Features: {tfidf_features_test_normal_70.shape}")
print(f"Under Sampling 80:20 - Train Features: {tfidf_features_us_80.shape}, Test Features: {tfidf_features_test_us_80.shape}")
print(f"Under Sampling 70:30 - Train Features: {tfidf_features_us_70.shape}, Test Features: {tfidf_features_test_us_70.shape}")
print(f"Over Sampling 80:20 - Train Features: {tfidf_features_os_80.shape}, Test Features: {tfidf_features_test_os_80.shape}")
print(f"Over Sampling 70:30 - Train Features: {tfidf_features_os_70.shape}, Test Features: {tfidf_features_test_os_70.shape}")
print(f"SMOTE 80:20 - Train Features: {tfidf_features_smote_80.shape}, Test Features: {labels_smote_80.shape}")
print(f"SMOTE 70:30 - Train Features: {tfidf_features_smote_70.shape}, Test Features: {labels_smote_70.shape}")

"""#Modelling

##SVM
"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.svm import LinearSVC

"""###SVM Normal Data 80:20"""

svm_normal_80 = LinearSVC(random_state=42)
svm_normal_80.fit(tfidf_features_normal_80, train_data_80['final_sentiment'])

#Prediksi pada data test
pred_svm_normal_80 = svm_normal_80.predict(tfidf_features_test_normal_80)

#Evaluasi model
print("SVM [Normal Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_svm_normal_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_svm_normal_80))

"""###SVM Under Sampling Data 80:20"""

svm_us_80 = LinearSVC(random_state=42)
svm_us_80.fit(tfidf_features_us_80, train_labels_us_80)

#Prediksi pada data test
pred_svm_us_80 = svm_us_80.predict(tfidf_features_test_us_80)

#Evaluasi model
print("SVM [Under Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_svm_us_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_svm_us_80))

"""###SVM Over Sampling Data 80:20"""

svm_os_80 = LinearSVC(random_state=42)
svm_os_80.fit(tfidf_features_os_80, train_labels_os_80)

#Prediksi pada data test
pred_svm_os_80 = svm_os_80.predict(tfidf_features_test_os_80)

#Evaluasi model
print("SVM [Over Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_svm_os_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_svm_os_80))

"""###SVM Normal Data 70:30"""

svm_normal_70 = LinearSVC(random_state=42)
svm_normal_70.fit(tfidf_features_normal_70, train_data_70['final_sentiment'])

#Prediksi pada data test
pred_svm_normal_70 = svm_normal_70.predict(tfidf_features_test_normal_70)

#Evaluasi model
print("SVM [Normal Data 70:30]")
print(classification_report(test_data_30['final_sentiment'], pred_svm_normal_70))
print("Akurasi Skenario:", accuracy_score(test_data_30['final_sentiment'], pred_svm_normal_70))

"""###SVM Under Sampling Data 70:30"""

svm_us_70 = LinearSVC(random_state=42)
svm_us_70.fit(tfidf_features_us_70, train_labels_us_70)

#Prediksi pada data test
pred_svm_us_70 = svm_us_70.predict(tfidf_features_test_us_70)

#Evaluasi model
print("SVM [Under Sampling Data 70:30]")
print(classification_report(test_data_30['final_sentiment'], pred_svm_us_70))
print("Akurasi Skenario:", accuracy_score(test_data_30['final_sentiment'], pred_svm_us_70))

"""###SVM Over Sampling Data 70:30"""

svm_os_70 = LinearSVC(random_state=42)
svm_os_70.fit(tfidf_features_os_70, train_labels_os_70)

#Prediksi pada data test
pred_svm_os_70 = svm_os_70.predict(tfidf_features_test_os_70)

#Evaluasi model
print("SVM [Over Sampling Data 70:30]")
print(classification_report(test_data_30['final_sentiment'], pred_svm_os_70))
print("Akurasi Skenario:", accuracy_score(test_data_30['final_sentiment'], pred_svm_os_70))

"""###SVM SMOTE 80:20"""

svm_smote_80 = LinearSVC(random_state=42)
svm_smote_80.fit(tfidf_features_smote_80, labels_smote_80)

#Prediksi pada data test
pred_svm_smote_80 = svm_smote_80.predict(tfidf_features_test_normal_80)

#Evaluasi model
print("SVM [SMOTE Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_svm_smote_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_svm_smote_80))

"""###SVM SMOTE 70:30"""

svm_smote_70 = LinearSVC(random_state=42)
svm_smote_70.fit(tfidf_features_smote_70, labels_smote_70)

#Prediksi pada data test
pred_svm_smote_70 = svm_smote_70.predict(tfidf_features_test_normal_70)

#Evaluasi model
print("SVM [SMOTE Data 70:30]")
print(classification_report(test_data_30['final_sentiment'], pred_svm_smote_70))
print("Akurasi Skenario:", accuracy_score(test_data_30['final_sentiment'], pred_svm_smote_70))

"""###Visualisasi Perbandingan Akurasi"""

import matplotlib.pyplot as plt
import numpy as np

accuracy_scores = {
    "SVM Normal Data 80:20": accuracy_score(test_data_20['final_sentiment'], pred_svm_normal_80),
    "SVM Under Sampling Data 80:20": accuracy_score(test_data_20['final_sentiment'], pred_svm_us_80),
    "SVM Over Sampling Data 80:20": accuracy_score(test_data_20['final_sentiment'], pred_svm_os_80),
    "SVM SMOTE 80:20": accuracy_score(test_data_20['final_sentiment'], pred_svm_smote_80),
    "SVM Normal Data 70:30": accuracy_score(test_data_30['final_sentiment'], pred_svm_normal_70),
    "SVM Under Sampling Data 70:30": accuracy_score(test_data_30['final_sentiment'], pred_svm_us_70),
    "SVM Over Sampling Data 70:30": accuracy_score(test_data_30['final_sentiment'], pred_svm_os_70),
    "SVM SMOTE 70:30": accuracy_score(test_data_30['final_sentiment'], pred_svm_smote_70),
}

# Membuat plot
labels = list(accuracy_scores.keys())
accuracies = list(accuracy_scores.values())

plt.figure(figsize=(15, 6))
bars = plt.bar(labels, accuracies, color='skyblue', alpha=0.8)

# Menampilkan nilai akurasi di atas tiap batang
for bar, accuracy in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 0.02,
             f"{accuracy:.2f}", ha='center', va='bottom', fontsize=12)

# Menambahkan detail plot
plt.title("Akurasi Model untuk Berbagai Skenario", fontsize=16)
plt.ylabel("Akurasi", fontsize=14)
plt.xlabel("Model", fontsize=14)
plt.ylim(0, 1)  # Rentang akurasi antara 0 dan 1
plt.xticks(rotation=15, fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Menampilkan plot
plt.tight_layout()
plt.show()

"""##Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

"""###NB NORMAL DATA 80 20"""

# Inisialisasi model Naive Bayes
nb_normal_80 = MultinomialNB()

# Melatih model pada data training
nb_normal_80.fit(tfidf_features_normal_80, train_data_80['final_sentiment'])

# Prediksi pada data test
pred_nb_normal_80 = nb_normal_80.predict(tfidf_features_test_normal_80)

# Evaluasi model
print("Naive Bayes [Normal Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_nb_normal_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_nb_normal_80))

"""###NB UNDERSAMPLING 80 20"""

# Inisialisasi model Naive Bayes
nb_us_80 = MultinomialNB()

# Melatih model pada data training
nb_us_80.fit(tfidf_features_us_80, train_labels_us_80)

# Prediksi pada data test
pred_nb_us_80 = nb_us_80.predict(tfidf_features_test_us_80)

# Evaluasi model
print("Naive Bayes [Under Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_nb_us_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_nb_us_80))

"""###NB OVERSAMPLING 80 20"""

# Inisialisasi model Naive Bayes
nb_os_80 = MultinomialNB()

# Melatih model pada data training
nb_os_80.fit(tfidf_features_os_80, train_labels_os_80)

# Prediksi pada data test
pred_nb_os_80 = nb_os_80.predict(tfidf_features_test_os_80)

# Evaluasi model
print("Naive Bayes [Over Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_nb_os_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_nb_os_80))

"""###NB NORMAL DATA 70 30"""

# Inisialisasi model Naive Bayes
nb_normal_70 = MultinomialNB()

# Melatih model pada data training
nb_normal_70.fit(tfidf_features_normal_70, train_data_70['final_sentiment'])

# Prediksi pada data test
pred_nb_normal_70 = nb_normal_70.predict(tfidf_features_test_normal_70)

# Evaluasi model
print("Naive Bayes [Normal Data 70:30]")
print(classification_report(test_data_30['final_sentiment'], pred_nb_normal_70))
print("Akurasi Skenario:", accuracy_score(test_data_30['final_sentiment'], pred_nb_normal_70))

"""###NB UNDERSAMPLING 70 30"""

# Inisialisasi model Naive Bayes
nb_us_70 = MultinomialNB()

# Melatih model pada data training
nb_us_70.fit(tfidf_features_us_70, train_labels_us_70)

# Prediksi pada data test
pred_nb_us_70 = nb_us_70.predict(tfidf_features_test_us_70)

# Evaluasi model
print("Naive Bayes [Under Sampling Data 70:30]")
print(classification_report(test_data_30['final_sentiment'], pred_nb_us_70))
print("Akurasi Skenario:", accuracy_score(test_data_30['final_sentiment'], pred_nb_us_70))

"""###NB OVERSAMPLING 70 30"""

# Inisialisasi model Naive Bayes
nb_os_70 = MultinomialNB()

# Melatih model pada data training
nb_os_70.fit(tfidf_features_os_70, train_labels_os_70)

# Prediksi pada data test
pred_nb_os_70 = nb_os_70.predict(tfidf_features_test_os_70)

# Evaluasi model
print("Naive Bayes [Over Sampling Data 70:30]")
print(classification_report(test_data_30['final_sentiment'], pred_nb_os_70))
print("Akurasi Skenario:", accuracy_score(test_data_30['final_sentiment'], pred_nb_os_70))

"""###NB SMOTE 80 20"""

# Inisialisasi model Naive Bayes
nb_smote_80 = MultinomialNB()

# Melatih model pada data training
nb_smote_80.fit(tfidf_features_smote_80, labels_smote_80)

# Prediksi pada data test
pred_nb_smote_80 = nb_smote_80.predict(tfidf_features_test_normal_80)

# Evaluasi model
print("Naive Bayes [SMOTE Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_nb_smote_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_nb_smote_80))

"""###NB SMOTE 70 30"""

# Inisialisasi model Naive Bayes
nb_smote_70 = MultinomialNB()

# Melatih model pada data training
nb_smote_70.fit(tfidf_features_smote_70, labels_smote_70)

# Prediksi pada data test
pred_nb_smote_70 = nb_smote_70.predict(tfidf_features_test_normal_70)

# Evaluasi model
print("Naive Bayes [SMOTE Data 70:30]")
print(classification_report(test_data_30['final_sentiment'], pred_nb_smote_70))
print("Akurasi Skenario:", accuracy_score(test_data_30['final_sentiment'], pred_nb_smote_70))

import matplotlib.pyplot as plt
import numpy as np

accuracy_scores = {
    "Naive Bayes Normal Data 80:20": accuracy_score(test_data_20['final_sentiment'], pred_nb_normal_80),
    "Naive Bayes Under Sampling Data 80:20": accuracy_score(test_data_20['final_sentiment'], pred_nb_us_80),
    "Naive Bayes Over Sampling Data 80:20": accuracy_score(test_data_20['final_sentiment'], pred_nb_os_80),
    "Naive Bayes SMOTE 80:20": accuracy_score(test_data_20['final_sentiment'], pred_nb_smote_80),
    "Naive Bayes Normal Data 70:30": accuracy_score(test_data_30['final_sentiment'], pred_nb_normal_70),
    "Naive Bayes Under Sampling Data 70:30": accuracy_score(test_data_30['final_sentiment'], pred_nb_us_70),
    "Naive Bayes Over Sampling Data 70:30": accuracy_score(test_data_30['final_sentiment'], pred_nb_os_70),
    "Naive Bayes SMOTE 70:30": accuracy_score(test_data_30['final_sentiment'], pred_nb_smote_70),
}

# Membuat plot
labels = list(accuracy_scores.keys())
accuracies = list(accuracy_scores.values())

plt.figure(figsize=(15, 6))
bars = plt.bar(labels, accuracies, color='skyblue', alpha=0.8)

# Menampilkan nilai akurasi di atas tiap batang
for bar, accuracy in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 0.02,
             f"{accuracy:.2f}", ha='center', va='bottom', fontsize=12)

# Menambahkan detail plot
plt.title("Akurasi Model untuk Berbagai Skenario", fontsize=16)
plt.ylabel("Akurasi", fontsize=14)
plt.xlabel("Model", fontsize=14)
plt.ylim(0, 1)  # Rentang akurasi antara 0 dan 1
plt.xticks(rotation=15, fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Menampilkan plot
plt.tight_layout()
plt.show()

"""##Random Forest (Hapus)"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Inisialisasi model Random Forest
rf_normal_80 = RandomForestClassifier(random_state=42)

# Melatih model pada data training
rf_normal_80.fit(tfidf_features_normal_80, train_data_80['final_sentiment'])

# Prediksi pada data test
pred_rf_normal_80 = rf_normal_80.predict(tfidf_features_test_normal_80)

# Evaluasi model
print("Random Forest [Normal Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_rf_normal_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_rf_normal_80))

# Inisialisasi model Random Forest
rf_us_80 = RandomForestClassifier(random_state=42)

# Melatih model pada data training
rf_us_80.fit(tfidf_features_us_80, train_labels_us_80)

# Prediksi pada data test
pred_rf_us_80 = rf_us_80.predict(tfidf_features_test_us_80)

# Evaluasi model
print("Random Forest [Under Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_rf_us_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_rf_us_80))

# Inisialisasi model Random Forest
rf_os_80 = RandomForestClassifier(random_state=42)

# Melatih model pada data training
rf_os_80.fit(tfidf_features_os_80, train_labels_os_80)

# Prediksi pada data test
pred_rf_os_80 = rf_os_80.predict(tfidf_features_test_os_80)

# Evaluasi model
print("Random Forest [Over Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_rf_os_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_rf_os_80))

"""##Logistic Regression (Hapus)"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Inisialisasi model Logistic Regression
lr_normal_80 = LogisticRegression(random_state=42, max_iter=1000)

# Melatih model pada data training
lr_normal_80.fit(tfidf_features_normal_80, train_data_80['final_sentiment'])

# Prediksi pada data test
pred_lr_normal_80 = lr_normal_80.predict(tfidf_features_test_normal_80)

# Evaluasi model
print("Logistic Regression [Normal Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_lr_normal_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_lr_normal_80))

# Inisialisasi model Logistic Regression
lr_us_80 = LogisticRegression(random_state=42, max_iter=1000)

# Melatih model pada data training
lr_us_80.fit(tfidf_features_us_80, train_labels_us_80)

# Prediksi pada data test
pred_lr_us_80 = lr_us_80.predict(tfidf_features_test_us_80)

# Evaluasi model
print("Logistic Regression [Under Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_lr_us_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_lr_us_80))

# Inisialisasi model Logistic Regression
lr_os_80 = LogisticRegression(random_state=42, max_iter=1000)

# Melatih model pada data training
lr_os_80.fit(tfidf_features_os_80, train_labels_os_80)

# Prediksi pada data test
pred_lr_os_80 = lr_os_80.predict(tfidf_features_test_os_80)

# Evaluasi model
print("Logistic Regression [Over Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_lr_os_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_lr_os_80))

"""##Polynomial Kernel (Hapus)"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Inisialisasi model SVM dengan kernel polinomial
svm_poly_normal_80 = SVC(kernel='poly', degree=3, random_state=42)

# Melatih model pada data training
svm_poly_normal_80.fit(tfidf_features_normal_80, train_data_80['final_sentiment'])

# Prediksi pada data test
pred_svm_poly_normal_80 = svm_poly_normal_80.predict(tfidf_features_test_normal_80)

# Evaluasi model
print("SVM Polynomial Kernel [Normal Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_svm_poly_normal_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_svm_poly_normal_80))

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Inisialisasi model SVM dengan kernel polinomial
svm_poly_us_80 = SVC(kernel='poly', degree=3, random_state=42)

# Melatih model pada data training
svm_poly_us_80.fit(tfidf_features_us_80, train_labels_us_80)

# Prediksi pada data test
pred_svm_poly_us_80 = svm_poly_us_80.predict(tfidf_features_test_us_80)

# Evaluasi model
print("SVM Polynomial Kernel [Under Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_svm_poly_us_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_svm_poly_us_80))

# Inisialisasi model SVM dengan kernel polinomial
svm_poly_os_80 = SVC(kernel='poly', degree=3, random_state=42)

# Melatih model pada data training
svm_poly_os_80.fit(tfidf_features_os_80, train_labels_os_80)

# Prediksi pada data test
pred_svm_poly_os_80 = svm_poly_os_80.predict(tfidf_features_test_os_80)

# Evaluasi model
print("SVM Polynomial Kernel [Over Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_svm_poly_os_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_svm_poly_os_80))

"""##Decision Tree (Hapus)"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score

# Inisialisasi model Decision Tree
dt_normal_80 = DecisionTreeClassifier(random_state=42)

# Melatih model pada data training
dt_normal_80.fit(tfidf_features_normal_80, train_data_80['final_sentiment'])

# Prediksi pada data test
pred_dt_normal_80 = dt_normal_80.predict(tfidf_features_test_normal_80)

# Evaluasi model
print("Decision Tree [Normal Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_dt_normal_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_dt_normal_80))

# Inisialisasi model Decision Tree
dt_us_80 = DecisionTreeClassifier(random_state=42)

# Melatih model pada data training
dt_us_80.fit(tfidf_features_us_80, train_labels_us_80)

# Prediksi pada data test
pred_dt_us_80 = dt_us_80.predict(tfidf_features_test_us_80)

# Evaluasi model
print("Decision Tree [Under Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_dt_us_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_dt_us_80))

# Inisialisasi model Decision Tree
dt_os_80 = DecisionTreeClassifier(random_state=42)

# Melatih model pada data training
dt_os_80.fit(tfidf_features_os_80, train_labels_os_80)

# Prediksi pada data test
pred_dt_os_80 = dt_os_80.predict(tfidf_features_test_os_80)

# Evaluasi model
print("Decision Tree [Over Sampling Data 80:20]")
print(classification_report(test_data_20['final_sentiment'], pred_dt_os_80))
print("Akurasi Skenario:", accuracy_score(test_data_20['final_sentiment'], pred_dt_os_80))

"""#Perbandingan Akurasi Berbagai Skenario"""

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 6))
bars = plt.bar(df_sorted['Model'], df_sorted['Akurasi'], color='skyblue')

# Menampilkan nilai akurasi
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom', fontsize=10)

plt.xlabel("Model", fontsize=14)
plt.ylabel("Akurasi", fontsize=14)
plt.title("Visualisasi Perbandingan Akurasi Berbagai Skenario Model (Sorted)", fontsize=16)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.tight_layout()
plt.show()

data = {
    'Model': ['SVM Normal Data 80:20', 'SVM Under Sampling Data 80:20', 'SVM Over Sampling Data 80:20', 'SVM SMOTE 80:20',
              'SVM Normal Data 70:30', 'SVM Under Sampling Data 70:30', 'SVM Over Sampling Data 70:30', 'SVM SMOTE 70:30',
              'Naive Bayes Normal Data 80:20', 'Naive Bayes Under Sampling Data 80:20', 'Naive Bayes Over Sampling Data 80:20', 'Naive Bayes SMOTE 80:20',
              'Naive Bayes Normal Data 70:30', 'Naive Bayes Under Sampling Data 70:30', 'Naive Bayes Over Sampling Data 70:30', 'Naive Bayes SMOTE 70:30'],
    'Akurasi': [accuracy_score(test_data_20['final_sentiment'], pred_svm_normal_80),
                accuracy_score(test_data_20['final_sentiment'], pred_svm_us_80),
                accuracy_score(test_data_20['final_sentiment'], pred_svm_os_80),
                accuracy_score(test_data_20['final_sentiment'], pred_svm_smote_80),
                accuracy_score(test_data_30['final_sentiment'], pred_svm_normal_70),
                accuracy_score(test_data_30['final_sentiment'], pred_svm_us_70),
                accuracy_score(test_data_30['final_sentiment'], pred_svm_os_70),
                accuracy_score(test_data_30['final_sentiment'], pred_svm_smote_70),
                accuracy_score(test_data_20['final_sentiment'], pred_nb_normal_80),
                accuracy_score(test_data_20['final_sentiment'], pred_nb_us_80),
                accuracy_score(test_data_20['final_sentiment'], pred_nb_os_80),
                accuracy_score(test_data_20['final_sentiment'], pred_nb_smote_80),
                accuracy_score(test_data_30['final_sentiment'], pred_nb_normal_70),
                accuracy_score(test_data_30['final_sentiment'], pred_nb_us_70),
                accuracy_score(test_data_30['final_sentiment'], pred_nb_os_70),
                accuracy_score(test_data_30['final_sentiment'], pred_nb_smote_70)]
}

df = pd.DataFrame(data)
df_sorted = df.sort_values(by='Akurasi', ascending=False)
df_sorted

"""##PICKLE"""

import pickle

# Simpan TF-IDF Vectorizer
with open('tfidf_vectorizer.pkl', 'wb') as tfidf_file:
       pickle.dump(tfidf_vectorizer_os_80, tfidf_file)

# Simpan Model SVM
with open('svm_model.pkl', 'wb') as model_file:
    pickle.dump(svm_os_80, model_file)

print("TF-IDF vectorizer and SVM model saved")

# Load TF-IDF Vectorizer
with open('tfidf_vectorizer.pkl', 'rb') as tfidf_file:
    tfidf_vectorizer = pickle.load(tfidf_file)

# Load Model SVM
with open('svm_model.pkl', 'rb') as model_file:
    svm_model = pickle.load(model_file)

print("TF-IDF vectorizer and SVM model mounted")

# Contoh Prediksi
text_input = ["Program ini sangat baik bagi negara!"]

# Transformasi teks dengan TF-IDF
text_features = tfidf_vectorizer.transform(text_input)

# Prediksi sentimen
sentiment_prediction = svm_model.predict(text_features)
print(f"Sentimen: {sentiment_prediction[0]}")

!pip install streamlit -q

!pip install Sastrawi

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import pickle
# import re
# import matplotlib.pyplot as plt
# from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
# 
# # Load pickle files (TF-IDF vectorizer and SVM model)
# with open('tfidf_vectorizer.pkl', 'rb') as tfidf_file:
#     tfidf_vectorizer = pickle.load(tfidf_file)
# 
# with open('svm_model.pkl', 'rb') as model_file:
#     svm_model = pickle.load(model_file)
# 
# # Load stopwords and stemmer
# stopword_factory = StopWordRemoverFactory()
# stopwords = set(stopword_factory.get_stop_words())
# stemmer_factory = StemmerFactory()
# stemmer = stemmer_factory.create_stemmer()
# 
# # Function for preprocessing text
# def preprocess_text(text):
#     # Remove mentions, hashtags, extra whitespace
#     text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # Remove mentions
#     text = re.sub(r'#\w+', '', text)  # Remove hashtags
#     text = re.sub(r'[^A-Za-z\s]', '', text)  # Remove non-alphabetic characters
#     text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
# 
#     # Case folding
#     text = text.lower()
# 
#     # Remove stopwords
#     text = ' '.join([word for word in text.split() if word not in stopwords])
# 
#     # Stemming
#     text = ' '.join([stemmer.stem(word) for word in text.split()])
# 
#     return text
# 
# #UI
# st.title("Sentiment Analysis Application")
# st.write("Enter text or upload a CSV file for sentiment analysis (positive or negative).")
# 
# # Tab navigation: Single Text Input or File Upload
# option = st.radio("Select input mode:", ("Text", "Upload CSV File"))
# 
# #Single Text Input
# if option == "Single Text":
#     user_input = st.text_area("Enter Text", placeholder="Type your text here...")
# 
#     if st.button("Analyze Sentiment"):
#         if user_input:
#             # Preprocess text
#             preprocessed_text = preprocess_text(user_input)
# 
#             # Extract features using TF-IDF
#             features = tfidf_vectorizer.transform([preprocessed_text])
# 
#             # Predict sentiment
#             prediction = svm_model.predict(features)[0]
# 
#             # Display result
#             if prediction == 'positif':
#                 st.success("Result: Positive Sentiment")
#             elif prediction == 'negatif':
#                 st.error("Result: Negative Sentiment")
#         else:
#             st.warning("Please enter some text to analyze!")
# 
# 
# #File Upload
# elif option == "Upload CSV File":
#     uploaded_file = st.file_uploader("Upload a CSV file", type=["csv"])
# 
#     if uploaded_file is not None:
#         try:
#             # Check file size (limit 50 MB)
#             if uploaded_file.size > 50 * 1024 * 1024:
#                 st.error("File is too large! Please upload a file smaller than 50 MB.")
#             else:
#                 # Load CSV file
#                 df = pd.read_csv(uploaded_file)
#                 st.write("Uploaded data:")
#                 st.dataframe(df.head())
# 
#                 # Validate column
#                 text_column = st.selectbox("Select text column:", df.columns)
#                 if st.button("Analyze CSV Sentiment"):
#                     # Preprocess all texts
#                     df['preprocessed_text'] = df[text_column].astype(str).apply(preprocess_text)
# 
#                     # Extract features using TF-IDF
#                     features = tfidf_vectorizer.transform(df['preprocessed_text'])
# 
#                     # Predict sentiments
#                     df['sentiment'] = svm_model.predict(features)
# 
#                     # Display results
#                     st.success("Analysis complete! Here are the results:")
#                     st.dataframe(df[[text_column, 'sentiment']])
# 
#                     # Visualize results with a pie chart
#                     sentiment_counts = df['sentiment'].value_counts()
#                     fig, ax = plt.subplots()
#                     ax.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90)
#                     ax.axis('equal')  # Equal aspect ratio ensures pie chart is circular.
#                     st.pyplot(fig)
# 
#                     # Download results as CSV
#                     csv = df.to_csv(index=False)
#                     st.download_button(
#                         label="Download Results as CSV",
#                         data=csv,
#                         file_name="sentiment_results.csv",
#                         mime="text/csv"
#                     )
#         except Exception as e:
#             st.error(f"An error occurred: {e}")
#

!wget -q -O - ipv4.icanhazip.com

!streamlit run app.py & npx localtunnel --port 8501

